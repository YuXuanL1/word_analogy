# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o5EW9uGBorU1AAiZKiIQKSW6gQx4x376

## Part I: Data Pre-processing
"""

import pandas as pd

# Download the Google Analogy dataset
!wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"
with open(f"{file_name}.txt", "r") as f:
    data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:10]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntatic` category.

questions = []
categories = []
sub_categories = []
n = 0
subcat = ""

for e in data:
  if e.startswith(":"):
    subcat = e
    n += 1
  elif e:
    questions.append(e)
    if n <= 5:
      categories.append('Semantic')
    else:
      categories.append('Syntatic')
    sub_categories.append(subcat)

# Create the dataframe
df = pd.DataFrame(
    {
        "Question": questions,
        "Category": categories,
        "SubCategory": sub_categories,
    }
)

df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

MODEL_NAME = "glove-wiki-gigaword-100"
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

# Load the pre-trained model (using GloVe vectors here)
model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
  try:
      # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      """ Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      """
      # 將Question的四個詞拆開
      word_a, word_b, word_c, word_d = analogy.split()
      # b + c - a
      result = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)

      preds.append(result[0][0])
      golds.append(word_d)

  except KeyError as e: # ChatGPT
        print(e) # 不在字典裡的詞
        preds.append(None)
        golds.append(word_d)

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_data = data[data['SubCategory'] == SUB_CATEGORY]
# 將Question的四個單詞拆開，並將重複的單詞示為同一個
family_words = set(family_data['Question'].str.split(expand=True).values.ravel())
# 將None移除
family_words.discard(None)

# 詞的向量
word_vectors = []
words = []
for w in family_words:
    # 確保該詞在model的字典中
    if w in model.key_to_index:
        word_vectors.append(model[w])
        words.append(w)

word_vectors = np.array(word_vectors)

# t-SNE
# 將數據降到二維以繪製成平面圖
tsne = TSNE(n_components=2, random_state=10)
tsne_results = tsne.fit_transform(word_vectors)

plt.figure(figsize=(12, 8))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1])

# 標上標籤
for i, label in enumerate(words):
    plt.annotate(label, (tsne_results[i, 0], tsne_results[i, 1]))

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.
"""

# Download the split Wikipedia files
# Each file contain 562365 lines (articles).
!gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz
!gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz
!gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz
!gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz
!gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz

# Download the split Wikipedia files
# Each file contain 562365 lines (articles), except the last file.
!gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz
!gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz
!gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz
!gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz
!gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz
!gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz

!gunzip -k wiki_texts_part_*.gz

# Combine the extracted wiki_texts_parts files.
!cat wiki_texts_part_*.txt > wiki_texts_combined.txt

# Check the first ten lines of the combined file
!head -n 10 wiki_texts_combined.txt

"""Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random

wiki_txt_path = "wiki_texts_combined.txt"
output_path = "output_20.txt"

# wiki_texts_combined.txt is a text file separated by linebreaks (\n).
# Each row in wiki_texts_combined.txt indicates a Wikipedia article.

with open(wiki_txt_path, "r", encoding="utf-8") as f:
    with open(output_path, "w", encoding="utf-8") as output_file:
    # TODO4: Sample `20%` Wikipedia articles
    # Write your code here

      all_articles = f.readlines()
      sample_size = int(len(all_articles) * 0.2)
      sampled_articles = random.sample(all_articles, sample_size) # claude
      output_file.writelines(sampled_articles)

# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec

# claude
# 下載必要的 NLTK 資源
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# claude
# 初始化停用詞列表和詞形還原器
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# claude
def preprocess_text(text):
    # 轉換為小寫
    text = text.lower()
    # 分詞
    tokens = word_tokenize(text)
    # 移除停用詞和詞形還原
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return tokens

def read_and_preprocess(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            yield preprocess_text(line.strip())

def read_raw_sentences(filepath):
    with open(filepath, 'r', encoding='utf-8') as file:
        for line in file:
            yield line.split()  # 直接按照空格分詞，不做額外處理

"""### model_2 (without preprocessing)"""

path = "output_20.txt"
raw_sentences = list(read_raw_sentences(path))

# without preprocess的Word2Vec模型
model_2 = Word2Vec(sentences=raw_sentences, vector_size=100, window=5, min_count=5, workers=4)

model_2.save("model_2.model")

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
  try:
      # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      """ Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      """
      word_a, word_b, word_c, word_d = analogy.split()
      # claude：使用 model_2.wv 來訪問 KeyedVectors
      word_vectors = model_2.wv
      result = word_vectors.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)

      preds.append(result[0][0])
      golds.append(word_d)

  except KeyError as e:
        print(e)
        preds.append(None)
        golds.append(word_d)

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_data = data[data['SubCategory'] == SUB_CATEGORY]
# 將Question的四個單詞拆開，並將重複的單詞示為同一個
family_words = set(family_data['Question'].str.split(expand=True).values.ravel())
# 將None移除
family_words.discard(None)

vectors = []
words = []
word_vectors = model_2.wv

for w in family_words:
    # 確保該詞在model的字典中
    if w in word_vectors.key_to_index:
        vectors.append(word_vectors[w])
        words.append(w)

vectors = np.array(vectors)

# t-SNE
# 將數據降到二維以繪製成平面圖
tsne = TSNE(n_components=2, random_state=10)
tsne_results = tsne.fit_transform(vectors)

plt.figure(figsize=(12, 8))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1])

# 標上標籤
for i, label in enumerate(words):
    plt.annotate(label, (tsne_results[i, 0], tsne_results[i, 1]))

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### model_1 (with preprocessing)"""

path = "output_20.txt"
sentences = list(read_and_preprocess(path))

# 訓練 Word2Vec 模型
model_1 = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=4)

model_1.save("model_1.model")

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
  try:
      # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      """ Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      """
      word_a, word_b, word_c, word_d = analogy.split()
      # 使用 model_1.wv 來訪問 KeyedVectors
      word_vectors = model_1.wv
      result = word_vectors.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)

      preds.append(result[0][0])
      golds.append(word_d)

  except KeyError as e:
        print(e)
        preds.append(None)
        golds.append(word_d)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_data = data[data['SubCategory'] == SUB_CATEGORY]
# 將Question的四個單詞拆開，並將重複的單詞示為同一個
family_words = set(family_data['Question'].str.split(expand=True).values.ravel())
# 將None移除
family_words.discard(None)

vectors = []
words = []
word_vectors = model_1.wv

for w in family_words:
    # 確保該詞在model的字典中
    if w in word_vectors.key_to_index:
        vectors.append(word_vectors[w])
        words.append(w)

vectors = np.array(vectors)

# t-SNE
# 將數據降到二維以繪製成平面圖
tsne = TSNE(n_components=2, random_state=10)
tsne_results = tsne.fit_transform(vectors)

plt.figure(figsize=(12, 8))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1])

# 標上標籤
for i, label in enumerate(words):
    plt.annotate(label, (tsne_results[i, 0], tsne_results[i, 1]))

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### model_3: training with 30% Wikipedia articles (without preprocessing)"""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random

wiki_txt_path = "wiki_texts_combined.txt"
output_path = "output_30.txt"

# wiki_texts_combined.txt is a text file separated by linebreaks (\n).
# Each row in wiki_texts_combined.txt indicates a Wikipedia article.

with open(wiki_txt_path, "r", encoding="utf-8") as f:
    with open(output_path, "w", encoding="utf-8") as output_file:
      all_articles = f.readlines()
      sample_size = int(len(all_articles) * 0.3)
      sampled_articles = random.sample(all_articles, sample_size)
      output_file.writelines(sampled_articles)

path = "output_30.txt"
raw_sentences = list(read_raw_sentences(path))

# without preprocess的Word2Vec模型
model_3 = Word2Vec(sentences=raw_sentences, vector_size=100, window=5, min_count=5, workers=4)

model_3.save("model_3.model")

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
  try:
      # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      """ Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      """
      word_a, word_b, word_c, word_d = analogy.split()
      # 使用 model_3.wv 來訪問 KeyedVectors
      word_vectors = model_3.wv
      result = word_vectors.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)

      preds.append(result[0][0])
      golds.append(word_d)

  except KeyError as e:
        print(e)
        preds.append(None)
        golds.append(word_d)

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_data = data[data['SubCategory'] == SUB_CATEGORY]
# 將Question的四個單詞拆開，並將重複的單詞示為同一個
family_words = set(family_data['Question'].str.split(expand=True).values.ravel())
# 將None移除
family_words.discard(None)

vectors = []
words = []
word_vectors = model_3.wv

for w in family_words:
    # 確保該詞在model的字典中
    if w in word_vectors.key_to_index:
        vectors.append(word_vectors[w])
        words.append(w)

vectors = np.array(vectors)

# t-SNE
# 將數據降到二維以繪製成平面圖
tsne = TSNE(n_components=2, random_state=10)
tsne_results = tsne.fit_transform(vectors)

plt.figure(figsize=(12, 8))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1])

# 標上標籤
for i, label in enumerate(words):
    plt.annotate(label, (tsne_results[i, 0], tsne_results[i, 1]))

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

